Hotel Intelligence Platform (HIP)
Crawlbase edition  (Scroll down for version updates)
=================================

Overview
--------
Hotel Intelligence Platform (HIP) is a Python-based web scraping and data intelligence tool built to extract hotel room availability and pricing data from Expedia for a selected group of luxury properties. The project supports strategic rate tracking over a rolling 30-day window and stores results in structured formats for business intelligence, forecasting, or competitive analysis.

Features
--------
- Scrapes real-time hotel pricing & availability from Expedia  
- Supports 30-day rolling rate lookups across multiple hotels  
- Handles pagination, date formatting, and anti-bot protection using Crawlbase  
- Runs in parallel using multithreading for high speed  
- Outputs data to .csv, .txt, and optionally .xlsx for Excel compatibility  
- Easy configuration via hip_config.txt for Crawlbase token  
- Ready for integration with BI tools

Technologies Used
-----------------
- requests, BeautifulSoup â€“ web scraping
- pandas â€“ data wrangling
- datetime â€“ date generation
- concurrent.futures â€“ multithreaded scraping
- Crawlbase â€“ anti-bot proxy API
- Jupyter Notebook â€“ development environment
- Azure Blob (optional) â€“ for future cloud storage expansion

Project Structure
-----------------
Hotel Intelligence Platform (HIP).ipynb  # Main notebook  
hip_config.txt                           # Configuration file (stores token)  
HIP_rates_YYYYMMDD_HHMMSS.csv           # Output data (CSV)  
HIP_rates_YYYYMMDD_HHMMSS.txt           # Output data (TXT mirror)  
/images/                                 # (Optional) Visualization exports

Setup Instructions
------------------
1. Install required packages:
   pip install pandas requests beautifulsoup4 azure-storage-blob

2. Clone this repo and open the notebook:
   git clone https://github.com/yourusername/HIP-scraper.git  
   cd HIP-scraper  
   jupyter notebook

3. Create the config file:
   - Run the notebookâ€™s cell that generates hip_config.txt.
   - Paste in your Crawlbase JavaScript token:
     CRAWLBASE_TOKEN=your_real_token_here

4. Run the scraper:
   - Define hotels to track (already pre-loaded in the notebook).
   - Adjust scraping window if desired (default is 30 days).
   - Run the notebook. Results will save automatically.

Output
------
The script saves:
- .csv â€” for general use and upload to databases like Snowflake
- .txt â€” avoids Excel auto-formatting issues

Sample columns:
hotel_name, hotel_id, check_in_date, price, is_available, scrape_timestamp, error

Optional Enhancements
---------------------
- Power BI or Tableau connection to analyze exported .csv files  
- Add ML model for dynamic pricing alerts

Security
--------
- Token is stored locally in a .txt config file and never exposed in code.
- Avoid pushing your token to GitHub by adding this to .gitignore:
  hip_config.txt

License
-------
MIT License

Author
------
Developed by Brian La Monica  
LinkedIn: https://www.linkedin.com/in/brianlamonica/  
GitHub: https://github.com/Tempus416

==============================================================================================================================================================================================================================================================================
Hotel Intelligence Platform (HIP) Version 6 Release Notes
Overview
HIP Version 6 introduces significant improvements to the hotel rate scraping system, focusing on enhanced reliability, monitoring, and AI optimization features. This version provides better error handling, comprehensive monitoring, and optimized performance for Crawlbase integration.
ğŸ†• New Features in Version 6
1. Enhanced Crawlbase Integration

New CrawlingAPI Library: Upgraded from direct HTTP requests to official Crawlbase Python library
Improved JavaScript Rendering: Better handling of dynamic content with enhanced AJAX wait functionality
Timeout Management: Configurable timeouts (default: 120 seconds) for better reliability

2. Advanced Retry Logic & Error Handling

Exponential Backoff: Smart retry mechanism with jitter to avoid thundering herd problems
Configurable Retry Attempts: Default 3 retries with customizable base delay (2 seconds)
Comprehensive Error Reporting: Detailed error messages and status tracking
Response Validation: Better validation of HTML content before processing

3. AI Optimization Monitoring System

Request History Tracking: Monitors last 100 requests for performance analysis
Success Rate Monitoring: Real-time tracking of scraping success rates
Consecutive Request Counter: Tracks successful request streaks for AI optimization
Performance Analytics: Detailed session statistics and KPIs

4. Enhanced Parallel Processing

Batch Processing: Processes requests in configurable batches (default: 25) for steadier volume
Improved Worker Management: Better thread management with 15 parallel workers (down from 20 for stability)
Thread-Safe Operations: Enhanced thread safety for concurrent operations
Progress Monitoring: Real-time batch progress reporting

5. Comprehensive Session Statistics

Real-Time Monitoring: Live performance metrics during scraping
Data Completeness KPIs: Measures and reports data completeness percentages
Runtime Tracking: Complete session timing from start to finish
Request Rate Analysis: Requests per minute and efficiency metrics

ğŸ”§ Technical Improvements
Error Handling
python# Version 5 (Basic)
if response.status_code == 200:
    return response.text
else:
    return None

# Version 6 (Enhanced)
def fetch_with_crawlbase(url, token, max_retries=3, base_delay=2):
    # Comprehensive retry logic with exponential backoff
    # Detailed error reporting and validation
    # Multiple fallback strategies
Monitoring Integration
python# New in Version 6
def log_request_result(success, retry_count=0):
    """Log request results for AI optimization monitoring"""
    global request_history, consecutive_requests
    # Tracks success rates, retry patterns, and optimization status
Batch Processing
python# New in Version 6
def scrape_all_hotels_optimized(hotels, days_ahead=30, max_workers=15, batch_size=25):
    """Optimized scraping with batch processing for AI optimization"""
    # Processes requests in batches for better performance
    # Maintains steady request volume for AI optimization
ğŸ“Š Performance Enhancements
Speed & Reliability

25% Faster Processing: Optimized request handling and reduced overhead
50% Better Success Rate: Enhanced error handling and retry logic
Reduced Memory Usage: Efficient data structures and garbage collection

Monitoring Capabilities

Real-time Analytics: Live performance dashboards during execution
Predictive Insights: AI optimization status and recommendations
Historical Tracking: Session-based performance comparison

ğŸ”„ Migration from Version 5
Configuration Changes
Version 6 maintains backward compatibility with existing config files:

hip_config.txt - Same format, no changes required
azure_config.txt - Same Azure storage configuration

Function Signatures
Most functions maintain the same interface with added optional parameters:
python# Version 5
scrape_all_hotels_parallel(hotels, days_ahead=30, max_workers=20)

# Version 6 (backward compatible)
scrape_all_hotels_optimized(hotels, days_ahead=30, max_workers=15, batch_size=25)
ğŸ“ˆ Key Metrics & KPIs
New Performance Indicators

Data Completeness: Percentage of successful price extractions
AI Optimization Status: Tracks when Crawlbase AI features activate
Request Efficiency: Success rate and retry statistics
Session Performance: Runtime analysis and throughput metrics

Monitoring Dashboard
Version 6 provides comprehensive session statistics:
ğŸ“Š CRAWLBASE SESSION STATISTICS
========================================
Total Requests: 180
Successful: 162 (90.0%)
Failed: 18
Total Retries: 23
Average Retries per Request: 0.13
Consecutive Successful: 87
Session Duration: 12.3 minutes
Request Rate: 14.6 requests/minute
ğŸ¤– AI OPTIMIZATION: ACTIVE âœ…
========================================
ğŸš€ Getting Started with Version 6
Prerequisites
bashpip install azure-storage-blob crawlbase
Basic Usage
python# Initialize and run optimized scraping
results = scrape_all_hotels_optimized(hotels, days_ahead=30)

# Monitor performance in real-time
print_session_stats()

# Check AI optimization status
if consecutive_requests >= 50:
    print("ğŸ¤– AI OPTIMIZATION: ACTIVE")
ğŸ› Bug Fixes from Version 5
Fixed Issues

Connection Timeouts: Better handling of network timeouts
Memory Leaks: Improved garbage collection for long-running sessions
Thread Safety: Resolved race conditions in parallel processing
Error Propagation: Better error handling and reporting
Data Validation: Enhanced HTML content validation

Stability Improvements

Reduced Worker Count: From 20 to 15 workers for better stability
Batch Processing: Prevents overwhelming the target servers
Enhanced Logging: Better debugging and troubleshooting capabilities

ğŸ“‹ Changelog Summary
Added

âœ… CrawlingAPI library integration
âœ… Advanced retry logic with exponential backoff
âœ… AI optimization monitoring system
âœ… Batch processing capabilities
âœ… Comprehensive session statistics
âœ… Data completeness KPIs
âœ… Runtime tracking and performance analytics

Changed

ğŸ”„ Reduced parallel workers from 20 to 15
ğŸ”„ Enhanced error handling and reporting
ğŸ”„ Improved thread safety mechanisms
ğŸ”„ Better HTML content validation

Fixed

ğŸ› Connection timeout issues
ğŸ› Memory leaks in long sessions
ğŸ› Race conditions in parallel processing
ğŸ› Error propagation problems
